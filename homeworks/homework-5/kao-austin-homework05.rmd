---
title: "Homework 4"
author: "Austin Kao"
output: pdf_document
header-includes:
    - \usepackage{commath}
font-size: 8px
---

# Hoff, 3.12 (Jeffrey's Prior)

a.
\begin{align*}
 p_J(\theta) &\propto \sqrt{I(\theta)}\\
 I(\theta) &= -E \left[ \frac{\partial^2 \text{log}(p(Y \mid \theta))}{\partial \theta^2} \mid \theta \right]\\
 p(Y \mid \theta) &= p(y \mid n, \theta) = \begin{pmatrix} n\\y \end{pmatrix} \theta^y (1-\theta)^{n-y}\\
 \frac{\partial \text{log}(p(Y \mid \theta))}{\partial \theta} &= \frac{y\theta^{y-1}(1-\theta)^{n-y} + (y-n)\theta^y(1-\theta)^{n-y-1}}{\theta^y(1-\theta)^{n-y}} = y\theta^{-1}+ (y-n)(1-\theta)^{-1}\\
 \frac{\partial^2 \text{log}(p(Y \mid \theta))}{\partial \theta^2} &= -\frac{y}{\theta^2} + \frac{y-n}{(1-\theta)^2}\\
 -E \left[ \frac{\partial^2 \text{log}(p(Y \mid \theta))}{\partial \theta^2} \mid \theta \right] &= -\left( E\left[ \frac{-y}{\theta^2} \mid \theta \right] + E\left[ \frac{y-n}{(1-\theta)^2} \mid \theta \right] \right) = -\left( \frac{1}{\theta^2} E[y \mid \theta] + \frac{1}{(1-\theta)^2} E[y \mid \theta] - \frac{n}{(1-\theta)^2} \right)\\
 E[y \mid \theta] &= n \theta\\
 -E \left[ \frac{\partial^2 \text{log}(p(Y \mid \theta))}{\partial \theta^2} \mid \theta \right] &= -\left( -\frac{n}{\theta} + \frac{n \theta}{(1-\theta)^2} - \frac{n}{(1-\theta)^2} \right) = \frac{n}{\theta} - \frac{n \theta - n}{(1-\theta)^2} = \frac{n}{\theta} + \frac{n}{1-\theta} = \frac{n}{\theta(1-\theta)}\\
 p_J(\theta) &\propto \sqrt{\frac{n}{\theta(1-\theta)}} = \sqrt{n}\theta^{-\frac{1}{2}}(1-\theta)^{-\frac{1}{2}}\\
 p_J(\theta) &\propto \theta^{\frac{1}{2} - 1}(1-\theta)^{\frac{1}{2} - 1}\\
 p_J(\theta) &\propto \text{Beta}\left(\frac{1}{2}, \frac{1}{2}\right)
\end{align*}
b.
\begin{align*}
 I(\psi) &= -E \left[ \frac{\partial^2 \text{log}(p(Y \mid \psi))}{\partial \psi^2} \mid \psi \right]\\
 p(Y \mid \psi) &= p(y \mid n, \psi) = \begin{pmatrix} n\\y \end{pmatrix} e^{\psi y} (1+ e^\psi)^{-n}\\
 \text{log}(p(Y \mid \psi)) &= \text{log} \begin{pmatrix} n\\y \end{pmatrix} + \psi y + \text{log}[(1+ e^\psi)^{-n}]\\
 \frac{\partial \text{log}(p(Y \mid \psi))}{\partial \psi} &= 0 + y + \frac{-ne^{\psi}(1-e^{\psi})^{-n-1}}{(1+e^\psi)^{-n}} = y -n e^\psi (1+ e^\psi)^{-1}\\
 \frac{\partial^2 \text{log}(p(Y \mid \psi))}{\partial \psi^2} &= 0-n \left( \frac{e^\psi}{1+ e^\psi} - \frac{e^{2\psi}}{(1+ e^\psi)^2} \right) = -n \left( \frac{e^\psi + e^{2\psi}}{(1+ e^\psi)^2} - \frac{e^{2\psi}}{(1+ e^\psi)^2} \right) = \frac{-n e^\psi}{(1+ e^\psi)^2}\\
 -E \left[ \frac{\partial^2 \text{log}(p(Y \mid \psi))}{\partial \psi^2} \mid \psi \right] &= - \left( \frac{-n e^\psi}{(1+ e^\psi)^2} \right) = \frac{n e^\psi}{(1+ e^\psi)^2}\\
 p_J(\psi) &\propto \sqrt{\frac{n e^\psi}{(1+ e^\psi)^2}} = \frac{\sqrt{n}\sqrt{e^\psi}}{1+ e^\psi}\\
 p_J(\psi) &\propto \frac{e^\frac{\psi}{2}}{1+ e^\psi}
\end{align*}
c.
\begin{align*}
  \psi &= \text{log} \left( \frac{\theta}{1-\theta} \right)\\
  \theta &= h(\psi) = \frac{e^\psi}{1+e^\psi}\\
  \frac{dh}{d\psi} &= \frac{e^\psi(1+e^\psi)-e^\psi(e^\psi)}{(1+e^\psi)^2} = \frac{e^\psi}{(1+e^\psi)^2}\\
  p_J(\theta) &\propto \theta^{\frac{1}{2} - 1}(1-\theta)^{\frac{1}{2} - 1}\\
  p_J(\psi) &= p_J(\frac{e^\psi}{1+e^\psi}) \times \abs{\frac{dh}{d\psi}} \propto \left( \frac{e^\psi}{1+e^\psi} \right)^{-\frac{1}{2}} \left(1-\frac{e^\psi}{1+e^\psi} \right)^{-\frac{1}{2}} \times \abs{\frac{e^\psi}{(1+e^\psi)^2}}\\
  p_J(\psi) &\propto \left( \frac{e^\psi}{1+e^\psi} \right)^{-\frac{1}{2}} \left(\frac{1}{1+e^\psi} \right)^{-\frac{1}{2}} \times \frac{e^\psi}{(1+e^\psi)^2} = \frac{e^{-\frac{\psi}{2}}}{(1+e^\psi)^{-1}} \times \frac{e^\psi}{(1+e^\psi)^2}\\
  p_J(\psi) &\propto \frac{e^\frac{\psi}{2}}{1+ e^\psi}
\end{align*}

# Lab Component


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
set.seed(1)
```

### Task 1

```{r}
# grid of points
x <- seq(0, 1, 10^-2)
```

```{r}
# Code from lab 5 rmd
fx <- function(x) sin(pi * x)^2
plot(fx, xlim = c(0,1), ylim = c(0,1.5), ylab = "f(x)", lwd = 2)
curve(dunif, add = TRUE, col = "blue", lwd = 2)
curve(dbeta(x,2,2), add = TRUE, col = "red", lwd = 2)
legend("bottom", legend = c(expression(paste("sin(",pi,"x)"^"2")),"Unif(0,1)",
"Beta(2,2)"), col = c("black", "blue", "red"), lty = c(1,1,1), bty = "n", cex = 1.1, lwd = 2)
```

### Tasks 2 -- 4

```{r}
# Code from lab 5 rmd
sim_fun <- function(f, envelope = "unif", par1 = 0, par2 = 1, n = 10^2, plot = TRUE){

  r_envelope <- match.fun(paste0("r", envelope))
  d_envelope <- match.fun(paste0("d", envelope))
  proposal <- r_envelope(n, par1, par2)
  density_ratio <- f(proposal) / d_envelope(proposal, par1, par2)
  samples <- proposal[runif(n) < density_ratio]
  acceptance_ratio <- length(samples) / n
  if (plot) {
    hist(samples, probability = TRUE,
         main = paste0("Histogram of ",
                       n, " samples from ",
                       envelope, "(", par1, ",", par2,
                       ").\n Acceptance ratio: ",
                       round(acceptance_ratio,2)),
                       cex.main = 0.75)
  }
  list(x = samples, acceptance_ratio = acceptance_ratio)
}
```

```{r}
# Code from lab 5 rmd
par(mfrow = c(2,2), mar = rep(4, 4))
unif_1 <- sim_fun(fx, envelope = "unif", par1 = 0, par2 = 1, n = 10^2)
unif_2 <- sim_fun(fx, envelope = "unif", par1 = 0, par2 = 1, n = 10^5)
# Beta(2,2) densities
beta_1 <- sim_fun(fx, envelope = "beta", par1 = 2, par2 = 2, n = 10^2)
beta_2 <- sim_fun(fx, envelope = "beta", par1 = 2, par2 = 2, n = 10^5)
```

Figure 2: Comparision of the output of the rejection sampling for 100 versus 100,000 simulations with uniform and beta distributions as envelope functions.

```{r}
# Code from lab 5 rmd
par(mfrow = c(1,1))
```

From what we can see above, the acceptance ratios for both distributions when sampling $10^5$ points are identical, but the acceptance ratio for the Beta(2,2) distribution is lower than that for the Unif(0,1) distribution when sampling $10^2$ points. This suggests that both distributions perform about equally well when approximating the normalized density of the posterior distribution when the posterior distribution is proportional to the sin$(\pi x)^2$ function.

### Task 5

The Beta(2,2) and the Unif(0,1) distributions are about the same, as seen in their nearly identical acceptance ratios. An advantage the Beta(2,2) distribution might have over the Unif(0,1) distribution is that its density has a shape similar to that of the sin$(\pi x)^2$ function.

```{r}
# Set seed for reproducibility
set.seed(123)
# Plot the N(0.5, 1/3) distribution compared to others
fx <- function(x) sin(pi * x)^2
plot(fx, xlim = c(0,1), ylim = c(0,1.5), ylab = "f(x)", lwd = 2)
curve(dunif, add = TRUE, col = "blue", lwd = 2)
curve(dbeta(x,2,2), add = TRUE, col = "red", lwd = 2)
curve(dnorm(x,0.5, 1/3), add = TRUE, col = "green", lwd = 2)
legend("bottom", legend = c(expression(paste("sin(",pi,"x)"^"2")),"Unif(0,1)",
"Beta(2,2)", "N(0.5,1/3)"), col = c("black", "blue", "red", "green"),
       lty = c(1,1,1,1), bty = "n", cex = 1.1, lwd = 2)
# Define a N(0.5, 1/3) enveloping function and simulate rejection sampling
norm_1 <- sim_fun(fx, envelope = "norm", par1 = 0.5, par2 = 1/3, n = 10^2)
norm_2 <- sim_fun(fx, envelope = "norm", par1 = 0.5, par2 = 1/3, n = 10^5)
```

An example of a better enveloping function is the Normal distribution with mean 0.5 and standard deviation $\frac{1}{3}$. This is because, as can be seen above, this Normal distribution is flatter than the Beta distribution and has a shape similar to that of the sin$(\pi x)^2$ function. Because the Normal distribution is flatter than the Beta distribution, there is a greater chance of accepting a sampled point, especially in the region where 0.4 < x < 0.6, where the area under the sin$(\pi x)^2$ function is greatest. This is confirmed by the acceptance ratios, which are higher for the N(0.5, 1/3) distribution than for the Beta(2,2) and the Unif(0,1) distributions. For example, when sampling $10^5$ samples, the acceptance ratio is 0.57 for N(0.5,1/3) vs. 0.5 for Unif(0,1) and Beta(2,2). This means that the N(0.5, 1/3) distribution does a better job approximating the normalized posterior distribution when the posterior distribution is proportional to the sin$(\pi x)^2$ function than the Unif(0,1) and Beta(2,2) distributions do.